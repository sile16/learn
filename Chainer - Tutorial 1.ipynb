{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "import sys\n",
    "# the mock-0.3.1 dir contains testcase.py, testutils.py & mock.py\n",
    "sys.path.append('/Users/roberm3/learn/chainer-1.8.1/examples/mnist')\n",
    "import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Done\n",
      "Converting training data...\n",
      "Done\n",
      "Converting test data...\n",
      "Done\n",
      "Save output...\n",
      "Done\n",
      "Convert completed\n"
     ]
    }
   ],
   "source": [
    "mnist = data.load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scale pixel values to 0 or 1\n",
    "x_all = mnist['data'].astype(np.float32) / 255\n",
    "y_all = mnist['target'].astype(np.int32)\n",
    "x_train, x_test = np.split(x_all, [60000])\n",
    "y_train, y_test = np.split(y_all, [60000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__(\n",
    "            l1=L.Linear(784,100),\n",
    "            l2=L.Linear(100,100),\n",
    "            l3=L.Linear(100,10))\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier(Chain):\n",
    "     def __init__(self, predictor):\n",
    "         super(Classifier, self).__init__(predictor=predictor)\n",
    "\n",
    "     def __call__(self, x, t):\n",
    "         y = self.predictor(x)\n",
    "         self.loss = F.softmax_cross_entropy(y, t)\n",
    "         self.accuracy = F.accuracy(y, t)\n",
    "         return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "model = L.Classifier(MLP())\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "\n",
    "batchsize = 100\n",
    "datasize = 60000\n",
    "for epoch in range(20):\n",
    "    print('epoch %d' % epoch)\n",
    "    indexes = np.random.permutation(datasize)\n",
    "    for i in range(0,datasize, batchsize):\n",
    "        x = Variable(x_train[indexes[i : i + batchsize]])\n",
    "        t = Variable(y_train[indexes[i : i + batchsize]])\n",
    "        optimizer.update(model, x, t)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 0.155032213274\n",
      "Mean Accu: 0.95490000546\n"
     ]
    }
   ],
   "source": [
    "sum_loss, sum_accuracy = 0, 0\n",
    "for i in range(0,10000,batchsize):\n",
    "    x = Variable(x_test[i: i + batchsize])\n",
    "    t = Variable(y_test[i: i + batchsize])\n",
    "    loss = model(x,t)\n",
    "    sum_loss += loss.data * batchsize\n",
    "    sum_accuracy += model.accuracy.data * batchsize\n",
    "    \n",
    "mean_loss = sum_loss / 10000\n",
    "mean_accuracy = sum_accuracy / 10000\n",
    "\n",
    "print(\"Mean Loss: {}\".format(mean_loss))\n",
    "print(\"Mean Accu: {}\".format(mean_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(0,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
